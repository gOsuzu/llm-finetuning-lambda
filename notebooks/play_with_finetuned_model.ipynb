{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuned LoRA Model Playground\n",
    "\n",
    "This notebook allows you to interact with your fine-tuned LoRA model and test its responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!uv add transformers torch accelerate python-dotenv peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# Check if token is available\n",
    "if hf_token:\n",
    "    print(\"‚úÖ Hugging Face token loaded successfully\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: HUGGINGFACE_TOKEN not found in environment variables\")\n",
    "    print(\"   Private models may not load properly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "ADAPTER_NAME = \"gOsuzu/RickQwen2.5-7B\"  # Your LoRA adapter\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"  # Base model\n",
    "\n",
    "print(f\"LoRA adapter: {ADAPTER_NAME}\")\n",
    "print(f\"Base model: {BASE_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lora_model(adapter_name, base_model_name, device_map=\"auto\", token=None):\n",
    "    \"\"\"Load LoRA adapter with base model\"\"\"\n",
    "    try:\n",
    "        print(f\"Loading base model: {base_model_name}\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"cpu\",  # First load to CPU to avoid device mapping issues\n",
    "            trust_remote_code=True,\n",
    "            token=token\n",
    "        )\n",
    "        \n",
    "        print(f\"Loading LoRA adapter: {adapter_name}\")\n",
    "        model = PeftModel.from_pretrained(base_model, adapter_name, token=token)\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"Moving model to GPU...\")\n",
    "            model = model.to(\"cuda\")\n",
    "        \n",
    "        # Load tokenizer from base model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            base_model_name, \n",
    "            trust_remote_code=True,\n",
    "            token=token\n",
    "        )\n",
    "        \n",
    "        # Set padding token if not set\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        print(f\"‚úÖ LoRA model loaded successfully!\")\n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading LoRA model: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned LoRA model\n",
    "finetuned_model, finetuned_tokenizer = load_lora_model(ADAPTER_NAME, BASE_MODEL, token=hf_token)\n",
    "\n",
    "# Alternative loading method if the above fails\n",
    "if finetuned_model is None:\n",
    "    print(\"\\nüîÑ Trying alternative loading method...\")\n",
    "    try:\n",
    "        # Load base model without device mapping\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            BASE_MODEL,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            token=hf_token\n",
    "        )\n",
    "        \n",
    "        # Load LoRA adapter\n",
    "        model = PeftModel.from_pretrained(base_model, ADAPTER_NAME, token=hf_token)\n",
    "        \n",
    "        # Move to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            BASE_MODEL, \n",
    "            trust_remote_code=True,\n",
    "            token=hf_token\n",
    "        )\n",
    "        \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        finetuned_model, finetuned_tokenizer = model, tokenizer\n",
    "        print(\"‚úÖ Alternative loading method successful!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Alternative loading also failed: {e}\")\n",
    "\n",
    "# Load base model for comparison (optional)\n",
    "# base_model, base_tokenizer = load_model(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_length=512, temperature=0.7):\n",
    "    \"\"\"Generate response from model\"\"\"\n",
    "    try:\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        \n",
    "        # Move to same device as model\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "                # Additioal safety parameters\n",
    "                num_beams=1,\n",
    "                early_stopping=True,\n",
    "                length_penalty=1.0,\n",
    "                no_repeat_ngram_size=3\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract only the new generated text\n",
    "        response = response[len(prompt):].strip()\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating response: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Êñ∞„Åó„ÅÑ„Çª„É´„ÇíËøΩÂä†„Åó„Å¶ÂÆüË°å\n",
    "def chat_with_model_safe(model, tokenizer, system_prompt=\"\", user_input=\"\"):\n",
    "    \"\"\"Chat with the model using safer generation parameters\"\"\"\n",
    "    \n",
    "    # Create chat template\n",
    "    chat_template = f\"\"\"<|im_start|>system\n",
    "{system_prompt}<|im_end|>\n",
    "<|im_start|>user\n",
    "{user_input}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(chat_template, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        \n",
    "        # Move to same device as model\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate with greedy decoding (safer)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=False,  # Greedy decoding - no probability issues\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = response[len(chat_template):].strip()\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# „ÉÜ„Çπ„ÉàÂÆüË°å\n",
    "if finetuned_model and finetuned_tokenizer:\n",
    "    system_prompt = \"You are Rick Sanchez from Rick and Morty. Respond in Rick's style.\"\n",
    "    user_input = \"Tell me about interdimensional travel.\"\n",
    "    \n",
    "    print(\"ü§ñ Rick's Response (Safe Method):\")\n",
    "    response = chat_with_model_safe(finetuned_model, finetuned_tokenizer, system_prompt, user_input)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Your Fine-tuned LoRA Model\n",
    "\n",
    "Now you can test your fine-tuned LoRA model with different prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Rick and Morty style conversation\n",
    "if finetuned_model and finetuned_tokenizer:\n",
    "    system_prompt = \"You are Rick Sanchez from Rick and Morty. Respond in Rick's style with his characteristic personality.\"\n",
    "    user_input = \"Tell me about interdimensional travel.\"\n",
    "    \n",
    "    print(\"ü§ñ Rick's Response:\")\n",
    "    response = chat_with_model(finetuned_model, finetuned_tokenizer, system_prompt, user_input)\n",
    "    print(response)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Another Rick and Morty prompt\n",
    "if finetuned_model and finetuned_tokenizer:\n",
    "    system_prompt = \"You are Rick Sanchez. Be sarcastic and brilliant like Rick.\"\n",
    "    user_input = \"What do you think about school?\"\n",
    "    \n",
    "    print(\"ü§ñ Rick's Response:\")\n",
    "    response = chat_with_model(finetuned_model, finetuned_tokenizer, system_prompt, user_input)\n",
    "    print(response)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Interactive chat\n",
    "def interactive_chat():\n",
    "    \"\"\"Interactive chat with the model\"\"\"\n",
    "    if not finetuned_model or not finetuned_tokenizer:\n",
    "        print(\"‚ùå Model not loaded!\")\n",
    "        return\n",
    "    \n",
    "    system_prompt = \"You are Rick Sanchez from Rick and Morty. Respond in Rick's style.\"\n",
    "    \n",
    "    print(\"ü§ñ Interactive Chat with Rick (type 'quit' to exit)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        response = chat_with_model(finetuned_model, finetuned_tokenizer, system_prompt, user_input)\n",
    "        print(f\"\\nRick: {response}\")\n",
    "\n",
    "# Uncomment the line below to start interactive chat\n",
    "# interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Information\n",
    "\n",
    "Display information about your fine-tuned LoRA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if finetuned_model:\n",
    "    print(\"üìä LoRA Model Information\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    # Total parameters\n",
    "    total_params = sum(p.numel() for p in finetuned_model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in finetuned_model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"Model Size: {total_params * 4 / 1024**3:.2f} GB (FP32)\")\n",
    "    \n",
    "    # Model configuration\n",
    "    print(f\"\\nLoRA Adapter: {ADAPTER_NAME}\")\n",
    "    print(f\"Base Model: {BASE_MODEL}\")\n",
    "    print(f\"Model Type: {type(finetuned_model).__name__}\")\n",
    "    \n",
    "    # Device information\n",
    "    device = next(finetuned_model.parameters()).device\n",
    "    print(f\"Device: {device}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
