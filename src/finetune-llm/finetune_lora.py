import os
import dotenv
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from constants import MAX_SEQ_LENGTH, MODEL_CONFIG, TRAINING_ARGS, LoRA_CONFIG

dotenv.load_dotenv()

def main():
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
    model_name = MODEL_CONFIG["model_name"]
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=MODEL_CONFIG["dtype"],
        device_map="auto",
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    
    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # LoRAè¨­å®š
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=LoRA_CONFIG["r"],
        lora_alpha=LoRA_CONFIG["lora_alpha"],
        lora_dropout=LoRA_CONFIG["lora_dropout"],
        target_modules=LoRA_CONFIG["target_modules"]
    )
    
    # PEFTãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
    model = get_peft_model(model, lora_config)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
    dataset = load_dataset("gOsuzu/rick-and-morty-transcripts-sharegpt", split="train")
    
    # ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é©ç”¨
    def apply_chat_template(example):
        chat_template = """<|im_start|>system
{SYSTEM}<|im_end|>
<|im_start|>user
{INPUT}<|im_end|>
<|im_start|>assistant
{OUTPUT}<|im_end|>"""
        
        conversations = example["conversations"]
        system_msg = conversations[0]["value"]
        user_msg = conversations[1]["value"]
        assistant_msg = conversations[2]["value"]
        
        formatted_text = chat_template.format(
            SYSTEM=system_msg,
            INPUT=user_msg,
            OUTPUT=assistant_msg
        )
        
        return {"text": formatted_text}
    
    processed_dataset = dataset.map(apply_chat_template)
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding=False,
            max_length=MAX_SEQ_LENGTH,
            return_tensors=None
        )
    
    tokenized_dataset = processed_dataset.map(tokenize_function, batched=True, remove_columns=processed_dataset.column_names)
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
    training_args = TrainingArguments(**TRAINING_ARGS)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
    )
    
    trainer.train()

    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    trainer.save_model()
    tokenizer.save_pretrained(TRAINING_ARGS["output_dir"])
    
    # Hugging Face Hubã«ãƒ—ãƒƒã‚·ãƒ¥
    huggingface_token = os.getenv("HUGGINGFACE_TOKEN")
    if not huggingface_token:
        print("âš ï¸  HUGGINGFACE_TOKENãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print("    .envãƒ•ã‚¡ã‚¤ãƒ«ã«HUGGINGFACE_TOKENã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        print("    ä¾‹: HUGGINGFACE_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx")
        return
    
    try:
        print("ğŸš€ Hugging Face Hubã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒƒã‚·ãƒ¥ä¸­...")
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒƒã‚·ãƒ¥
        model.push_to_hub(
            "gOsuzu/RickQwen2.5-7B",
            token=huggingface_token,
            private=True,  # éå…¬é–‹ãƒªãƒã‚¸ãƒˆãƒª
        )
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ—ãƒƒã‚·ãƒ¥
        tokenizer.push_to_hub(
            "gOsuzu/RickQwen2.5-7B",
            token=huggingface_token,
            private=True,  # éå…¬é–‹ãƒªãƒã‚¸ãƒˆãƒª
        )
        
        print("âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("âœ… ãƒ¢ãƒ‡ãƒ«ãŒHugging Face Hubã«ãƒ—ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã—ãŸ: gOsuzu/RickQwen2.5-7B")
        print("ğŸŒ ãƒ¢ãƒ‡ãƒ«URL: https://huggingface.co/gOsuzu/RickQwen2.5-7B")
        
    except Exception as e:
        print(f"âŒ Hugging Face Hubã¸ã®ãƒ—ãƒƒã‚·ãƒ¥ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        print("ğŸ’¡ ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ ./rick-llm-output ã«ã‚ã‚Šã¾ã™")
        print("ğŸ’¡ æ‰‹å‹•ã§ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print("   cd ./rick-llm-output")
        print("   huggingface-cli login")
        print("   huggingface-cli repo create gOsuzu/RickQwen2.5-7B --type model")
        print("   git add . && git commit -m 'Add model' && git push")

if __name__ == "__main__":
    main() 