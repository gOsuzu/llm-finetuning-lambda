import os
import dotenv
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from constants import MAX_SEQ_LENGTH, MODEL_CONFIG, TRAINING_ARGS, LoRA_CONFIG, COMET_CONFIG
import comet_ml

dotenv.load_dotenv()

def setup_comet_experiment():
    """Setup Comet ML experiment for tracking"""
    try:
        # Get Comet API key from environment
        comet_api_key = os.getenv("COMET_API_KEY")
        if not comet_api_key:
            print("Warning: COMET_API_KEY not found in environment variables. Comet tracking will be disabled.")
            return None
        
        # Initialize Comet experiment
        experiment = comet_ml.Experiment(
            api_key=comet_api_key,
            project_name=COMET_CONFIG["project_name"],
            workspace=COMET_CONFIG["workspace"],
            experiment_name=COMET_CONFIG["experiment_name"],
            log_code=COMET_CONFIG["log_code"],
            log_parameters=COMET_CONFIG["log_parameters"],
            log_metrics=COMET_CONFIG["log_metrics"],
            log_histograms=COMET_CONFIG["log_histograms"],
            log_gradients=COMET_CONFIG["log_gradients"],
        )
        
        # Log model and training configuration
        experiment.log_parameters({
            "model_name": MODEL_CONFIG["model_name"],
            "max_seq_length": MAX_SEQ_LENGTH,
            **LoRA_CONFIG,
            **TRAINING_ARGS
        })
        
        print(f"Comet experiment initialized: {experiment.get_key()}")
        return experiment
        
    except Exception as e:
        print(f"Error setting up Comet experiment: {e}")
        return None

def main():
    # Setup Comet experiment
    experiment = setup_comet_experiment()
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
    model_name = MODEL_CONFIG["model_name"]
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=MODEL_CONFIG["dtype"],
        device_map="auto",
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    
    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # LoRAè¨­å®š
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=LoRA_CONFIG["r"],
        lora_alpha=LoRA_CONFIG["lora_alpha"],
        lora_dropout=LoRA_CONFIG["lora_dropout"],
        target_modules=LoRA_CONFIG["target_modules"]
    )
    
    # PEFTãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
    model = get_peft_model(model, lora_config)
    
    # Log model info to Comet
    if experiment:
        experiment.log_parameter("total_parameters", sum(p.numel() for p in model.parameters()))
        experiment.log_parameter("trainable_parameters", sum(p.numel() for p in model.parameters() if p.requires_grad))
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
    dataset = load_dataset("gOsuzu/rick-and-morty-transcripts-sharegpt", split="train")
    
    # Log dataset info to Comet
    if experiment:
        experiment.log_parameter("dataset_size", len(dataset))
        experiment.log_parameter("dataset_name", "gOsuzu/rick-and-morty-transcripts-sharegpt")
    
    # ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é©ç”¨
    def apply_chat_template(example):
        chat_template = """<|im_start|>system
{SYSTEM}<|im_end|>
<|im_start|>user
{INPUT}<|im_end|>
<|im_start|>assistant
{OUTPUT}<|im_end|>"""
        
        conversations = example["conversations"]
        system_msg = conversations[0]["value"]
        user_msg = conversations[1]["value"]
        assistant_msg = conversations[2]["value"]
        
        formatted_text = chat_template.format(
            SYSTEM=system_msg,
            INPUT=user_msg,
            OUTPUT=assistant_msg
        )
        
        return {"text": formatted_text}
    
    processed_dataset = dataset.map(apply_chat_template)
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding=False,
            max_length=MAX_SEQ_LENGTH,
            return_tensors=None
        )
    
    tokenized_dataset = processed_dataset.map(tokenize_function, batched=True, remove_columns=processed_dataset.column_names)
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
    training_args = TrainingArguments(**TRAINING_ARGS)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )
    
    # Custom callback for additional Comet logging
    class CometCallback:
        def __init__(self, experiment):
            self.experiment = experiment
            
        def on_log(self, args, state, control, logs=None, **kwargs):
            if logs and self.experiment:
                # Log additional metrics
                for key, value in logs.items():
                    if isinstance(value, (int, float)):
                        self.experiment.log_metric(key, value, step=state.global_step)
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
        callbacks=[CometCallback(experiment)] if experiment else None,
    )
    
    print("Starting training with Comet ML tracking...")
    trainer.train()
    
    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    trainer.save_model()
    tokenizer.save_pretrained(TRAINING_ARGS["output_dir"])
    
    # Log final model info
    if experiment:
        experiment.log_parameter("training_completed", True)
        experiment.log_parameter("final_loss", trainer.state.log_history[-1].get("loss", "N/A") if trainer.state.log_history else "N/A")
        experiment.end()
        print(f"Training completed! View results at: {experiment.get_url()}")
    
    # Hugging Face Hubã«ãƒ—ãƒƒã‚·ãƒ¥
    huggingface_token = os.getenv("HUGGINGFACE_TOKEN")
    if not huggingface_token:
        print("âš ï¸  HUGGINGFACE_TOKENãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print("    .envãƒ•ã‚¡ã‚¤ãƒ«ã«HUGGINGFACE_TOKENã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        print("    ä¾‹: HUGGINGFACE_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx")
        return
    
    try:
        print("ğŸš€ Hugging Face Hubã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒƒã‚·ãƒ¥ä¸­...")
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒƒã‚·ãƒ¥
        model.push_to_hub(
            "gOsuzu/RickQwen2.5-7B",
            token=huggingface_token,
            private=True,  # éå…¬é–‹ãƒªãƒã‚¸ãƒˆãƒª
        )
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ—ãƒƒã‚·ãƒ¥
        tokenizer.push_to_hub(
            "gOsuzu/RickQwen2.5-7B",
            token=huggingface_token,
            private=True,  # éå…¬é–‹ãƒªãƒã‚¸ãƒˆãƒª
        )
        
        print("âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("âœ… ãƒ¢ãƒ‡ãƒ«ãŒHugging Face Hubã«ãƒ—ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã—ãŸ: gOsuzu/RickQwen2.5-7B")
        print("ğŸŒ ãƒ¢ãƒ‡ãƒ«URL: https://huggingface.co/gOsuzu/RickQwen2.5-7B")
        
    except Exception as e:
        print(f"âŒ Hugging Face Hubã¸ã®ãƒ—ãƒƒã‚·ãƒ¥ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        print("ğŸ’¡ ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ ./rick-llm-output ã«ã‚ã‚Šã¾ã™")
        print("ğŸ’¡ æ‰‹å‹•ã§ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print("   cd ./rick-llm-output")
        print("   huggingface-cli login")
        print("   huggingface-cli repo create gOsuzu/RickQwen2.5-7B --type model")
        print("   git add . && git commit -m 'Add model' && git push")

if __name__ == "__main__":
    main() 